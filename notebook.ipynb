{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Supervised Learning with scikit-learn\n",
                "\n",
                "## Classification\n",
                "\n",
                "### k-Nearest Neighbors: Fit\n",
                "\n",
                "In this exercise, you will build your first classification model using\n",
                "the `churn_df` dataset, which has been preloaded for the remainder of\n",
                "the chapter.\n",
                "\n",
                "The target, `\"churn\"`, needs to be a single column with the same number\n",
                "of observations as the feature data. The feature data has already been\n",
                "converted into `numpy` arrays.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `KNeighborsClassifier` from `sklearn.neighbors`.\n",
                "- Instantiate a `KNeighborsClassifier` called `knn` with `6` neighbors.\n",
                "- Fit the classifier to the data using the `.fit()` method.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import KNeighborsClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier \n",
                "\n",
                "y = churn_df[\"churn\"].values\n",
                "X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n",
                "\n",
                "# Create a KNN classifier with 6 neighbors\n",
                "knn = KNeighborsClassifier(n_neighbors=6)\n",
                "\n",
                "# Fit the classifier to the data\n",
                "knn.fit(X, y)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### k-Nearest Neighbors: Predict\n",
                "\n",
                "Now you have fit a KNN classifier, you can use it to predict the label\n",
                "of new data points. All available data was used for training, however,\n",
                "fortunately, there are new observations available. These have been\n",
                "preloaded for you as `X_new`.\n",
                "\n",
                "The model `knn`, which you created and fit the data in the last\n",
                "exercise, has been preloaded for you. You will use your classifier to\n",
                "predict the labels of a set of new data points:\n",
                "\n",
                "    X_new = np.array([[30.0, 17.5],\n",
                "                      [107.0, 24.1],\n",
                "                      [213.0, 10.9]])\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `y_pred` by predicting the target values of the unseen features\n",
                "  `X_new`.\n",
                "- Print the predicted labels for the set of predictions.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the labels for the X_new\n",
                "y_pred = knn.predict(X_new)\n",
                "\n",
                "# Print the predictions\n",
                "print(\"Predictions: {}\".format(y_pred)) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train/test split + computing accuracy\n",
                "\n",
                "It's time to practice splitting your data into training and test sets\n",
                "with the `churn_df` dataset!\n",
                "\n",
                "NumPy arrays have been created for you containing the features as `X`\n",
                "and the target variable as `y`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `train_test_split` from `sklearn.model_selection`.\n",
                "- Split `X` and `y` into training and test sets, setting `test_size`\n",
                "  equal to 20%, `random_state` to `42`, and ensuring the target label\n",
                "  proportions reflect that of the original dataset.\n",
                "- Fit the `knn` model to the training data.\n",
                "- Compute and print the model's accuracy for the test data.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the module\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X = churn_df.drop(\"churn\", axis=1).values\n",
                "y = churn_df[\"churn\"].values\n",
                "\n",
                "# Split into training and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "knn = KNeighborsClassifier(n_neighbors=5)\n",
                "\n",
                "# Fit the classifier to the training data\n",
                "knn.fit(X_train, y_train)\n",
                "\n",
                "# Print the accuracy\n",
                "print(knn.score(X_test, y_test))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Overfitting and underfitting\n",
                "\n",
                "Interpreting model complexity is a great way to evaluate supervised\n",
                "learning performance. Your aim is to produce a model that can interpret\n",
                "the relationship between features and the target variable, as well as\n",
                "generalize well when exposed to new observations.\n",
                "\n",
                "The training and test sets have been created from the `churn_df` dataset\n",
                "and preloaded as `X_train`, `X_test`, `y_train`, and `y_test`.\n",
                "\n",
                "In addition, `KNeighborsClassifier` has been imported for you along with\n",
                "`numpy` as `np`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `neighbors` as a `numpy` array of values from `1` up to and\n",
                "  including `12`.\n",
                "- Instantiate a KNN classifier, with the number of neighbors equal to\n",
                "  the `neighbor` iterator.\n",
                "- Fit the model to the training data.\n",
                "- Calculate accuracy scores for the training set and test set separately\n",
                "  using the `.score()` method, and assign the results to the index of\n",
                "  the `train_accuracies` and `test_accuracies` dictionaries,\n",
                "  respectively.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create neighbors\n",
                "neighbors = np.arange(1, 13)\n",
                "train_accuracies = {}\n",
                "test_accuracies = {}\n",
                "\n",
                "for neighbor in neighbors:\n",
                "  \n",
                "  \t# Set up a KNN Classifier\n",
                "  \tknn = KNeighborsClassifier(n_neighbors=neighbor)\n",
                "  \n",
                "  \t#Â Fit the model\n",
                "  \tknn.fit(X_train, y_train)\n",
                "  \n",
                "  \t# Compute accuracy\n",
                "  \ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n",
                "  \ttest_accuracies[neighbor] = knn.score(X_test, y_test)\n",
                "print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing model complexity\n",
                "\n",
                "Now you have calculated the accuracy of the KNN model on the training\n",
                "and test sets using various values of `n_neighbors`, you can create a\n",
                "model complexity curve to visualize how performance changes as the model\n",
                "becomes less complex!\n",
                "\n",
                "The variables `neighbors`, `train_accuracies`, and `test_accuracies`,\n",
                "which you generated in the previous exercise, have all been preloaded\n",
                "for you. You will plot the results to aid in finding the optimal number\n",
                "of neighbors for your model.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Add a title `\"KNN: Varying Number of Neighbors\"`.\n",
                "- Plot the `.values()` method of `train_accuracies` on the y-axis\n",
                "  against `neighbors` on the x-axis, with a label of\n",
                "  `\"Training Accuracy\"`.\n",
                "- Plot the `.values()` method of `test_accuracies` on the y-axis against\n",
                "  `neighbors` on the x-axis, with a label of `\"Testing Accuracy\"`.\n",
                "- Display the plot.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add a title\n",
                "plt.title(\"KNN: Varying Number of Neighbors\")\n",
                "\n",
                "#Â Plot training accuracies\n",
                "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
                "\n",
                "# Plot test accuracies\n",
                "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
                "\n",
                "plt.legend()\n",
                "plt.xlabel(\"Number of Neighbors\")\n",
                "plt.ylabel(\"Accuracy\")\n",
                "\n",
                "# Display the plot\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regression\n",
                "\n",
                "### Creating features\n",
                "\n",
                "In this chapter, you will work with a dataset called `sales_df`, which\n",
                "contains information on advertising campaign expenditure across\n",
                "different media types, and the number of dollars generated in sales for\n",
                "the respective campaign. The dataset has been preloaded for you. Here\n",
                "are the first two rows:\n",
                "\n",
                "         tv        radio      social_media    sales\n",
                "    1    13000.0   9237.76    2409.57         46677.90\n",
                "    2    41000.0   15886.45   2913.41         150177.83\n",
                "\n",
                "You will use the advertising expenditure as features to predict sales\n",
                "values, initially working with the `\"radio\"` column. However, before you\n",
                "make any predictions you will need to create the feature and target\n",
                "arrays, reshaping them to the correct format for scikit-learn.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `X`, an array of the values from the `sales_df` DataFrame's\n",
                "  `\"radio\"` column.\n",
                "- Create `y`, an array of the values from the `sales_df` DataFrame's\n",
                "  `\"sales\"` column.\n",
                "- Reshape `X` into a two-dimensional NumPy array.\n",
                "- Print the shape of `X` and `y`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Create X from the radio column's values\n",
                "X = sales_df[\"radio\"].values\n",
                "\n",
                "# Create y from the sales column's values\n",
                "y = sales_df[\"sales\"].values\n",
                "\n",
                "# Reshape X\n",
                "X = X.reshape(-1, 1)\n",
                "\n",
                "# Check the shape of the features and targets\n",
                "print(X.shape, y.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Building a linear regression model\n",
                "\n",
                "Now you have created your feature and target arrays, you will train a\n",
                "linear regression model on all feature and target values.\n",
                "\n",
                "As the goal is to assess the relationship between the feature and target\n",
                "values there is no need to split the data into training and test sets.\n",
                "\n",
                "`X` and `y` have been preloaded for you as follows:\n",
                "\n",
                "    y = sales_df[\"sales\"].values\n",
                "    X = sales_df[\"radio\"].values.reshape(-1, 1)\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `LinearRegression`.\n",
                "- Instantiate a linear regression model.\n",
                "- Predict sales values using `X`, storing as `predictions`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import LinearRegression\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Create the model\n",
                "reg = LinearRegression()\n",
                "\n",
                "# Fit the model to the data\n",
                "reg.fit(X, y)\n",
                "\n",
                "# Make predictions\n",
                "predictions = reg.predict(X)\n",
                "\n",
                "print(predictions[:5])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing a linear regression model\n",
                "\n",
                "Now you have built your linear regression model and trained it using all\n",
                "available observations, you can visualize how well the model fits the\n",
                "data. This allows you to interpret the relationship between `radio`\n",
                "advertising expenditure and `sales` values.\n",
                "\n",
                "The variables `X`, an array of `radio` values, `y`, an array of `sales`\n",
                "values, and `predictions`, an array of the model's predicted values for\n",
                "`y` given `X`, have all been preloaded for you from the previous\n",
                "exercise.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `matplotlib.pyplot` as `plt`.\n",
                "- Create a scatter plot visualizing `y` against `X`, with observations\n",
                "  in blue.\n",
                "- Draw a red line plot displaying the predictions against `X`.\n",
                "- Display the plot.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import matplotlib.pyplot\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create scatter plot\n",
                "plt.scatter(X, y, color=\"blue\")\n",
                "\n",
                "# Create line plot\n",
                "plt.plot(X, predictions, color=\"red\")\n",
                "plt.xlabel(\"Radio Expenditure ($)\")\n",
                "plt.ylabel(\"Sales ($)\")\n",
                "\n",
                "# Display the plot\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fit and predict for regression\n",
                "\n",
                "Now you have seen how linear regression works, your task is to create a\n",
                "multiple linear regression model using all of the features in the\n",
                "`sales_df` dataset, which has been preloaded for you. As a reminder,\n",
                "here are the first two rows:\n",
                "\n",
                "         tv        radio      social_media    sales\n",
                "    1    13000.0   9237.76    2409.57         46677.90\n",
                "    2    41000.0   15886.45   2913.41         150177.83\n",
                "\n",
                "You will then use this model to predict sales based on the values of the\n",
                "test features.\n",
                "\n",
                "`LinearRegression` and `train_test_split` have been preloaded for you\n",
                "from their respective modules.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `X`, an array containing values of all features in `sales_df`,\n",
                "  and `y`, containing all values from the `\"sales\"` column.\n",
                "- Instantiate a linear regression model.\n",
                "- Fit the model to the training data.\n",
                "- Create `y_pred`, making predictions for `sales` using the test\n",
                "  features.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create X and y arrays\n",
                "X = sales_df.drop(\"sales\", axis=1).values\n",
                "y = sales_df[\"sales\"].values\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# Instantiate the model\n",
                "reg = LinearRegression()\n",
                "\n",
                "# Fit the model to the data\n",
                "reg.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = reg.predict(X_test)\n",
                "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Regression performance\n",
                "\n",
                "Now you have fit a model, `reg`, using all features from `sales_df`, and\n",
                "made predictions of sales values, you can evaluate performance using\n",
                "some common regression metrics.\n",
                "\n",
                "The variables `X_train`, `X_test`, `y_train`, `y_test`, and `y_pred`,\n",
                "along with the fitted model, `reg`, all from the last exercise, have\n",
                "been preloaded for you.\n",
                "\n",
                "Your task is to find out how well the features can explain the variance\n",
                "in the target values, along with assessing the model's ability to make\n",
                "predictions on unseen data.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `mean_squared_error`.\n",
                "- Calculate the model's R-squared score by passing the test feature\n",
                "  values and the test target values to an appropriate method.\n",
                "- Calculate the model's root mean squared error using `y_test` and\n",
                "  `y_pred`.\n",
                "- Print `r_squared` and `rmse`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import mean_squared_error\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# Compute R-squared\n",
                "r_squared = reg.score(X_test, y_test)\n",
                "\n",
                "# Compute RMSE\n",
                "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
                "\n",
                "# Print the metrics\n",
                "print(\"R^2: {}\".format(r_squared))\n",
                "print(\"RMSE: {}\".format(rmse))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cross-validation for R-squared\n",
                "\n",
                "Cross-validation is a vital approach to evaluating a model. It maximizes\n",
                "the amount of data that is available to the model, as the model is not\n",
                "only trained but also tested on all of the available data.\n",
                "\n",
                "In this exercise, you will build a linear regression model, then use\n",
                "6-fold cross-validation to assess its accuracy for predicting sales\n",
                "using social media advertising expenditure. You will display the\n",
                "individual score for each of the six-folds.\n",
                "\n",
                "The `sales_df` dataset has been split into `y` for the target variable,\n",
                "and `X` for the features, and preloaded for you. `LinearRegression` has\n",
                "been imported from `sklearn.linear_model`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `KFold` and `cross_val_score`.\n",
                "- Create `kf` by calling `KFold()`, setting the number of splits to six,\n",
                "  `shuffle` to `True`, and setting a seed of `5`.\n",
                "- Perform cross-validation using `reg` on `X` and `y`, passing `kf` to\n",
                "  `cv`.\n",
                "- Print the `cv_scores`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the necessary modules\n",
                "from sklearn.model_selection import KFold, cross_val_score\n",
                "\n",
                "#Â Create a KFold object\n",
                "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
                "\n",
                "reg = LinearRegression()\n",
                "\n",
                "# Compute 6-fold cross-validation scores\n",
                "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
                "\n",
                "# Print scores\n",
                "print(cv_scores)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyzing cross-validation metrics\n",
                "\n",
                "Now you have performed cross-validation, it's time to analyze the\n",
                "results.\n",
                "\n",
                "You will display the mean, standard deviation, and 95% confidence\n",
                "interval for `cv_results`, which has been preloaded for you from the\n",
                "previous exercise.\n",
                "\n",
                "`numpy` has been imported for you as `np`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Calculate and print the mean of the results.\n",
                "- Calculate and print the standard deviation of `cv_results`.\n",
                "- Display the 95% confidence interval for your results using\n",
                "  `np.quantile()`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print the mean\n",
                "print(np.mean(cv_results))\n",
                "\n",
                "# Print the standard deviation\n",
                "print(np.std(cv_results))\n",
                "\n",
                "# Print the 95% confidence interval\n",
                "print(np.quantile(cv_results, [0.025, 0.975]))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Regularized regression: Ridge\n",
                "\n",
                "Ridge regression performs regularization by computing the *squared*\n",
                "values of the model parameters multiplied by alpha and adding them to\n",
                "the loss function.\n",
                "\n",
                "In this exercise, you will fit ridge regression models over a range of\n",
                "different alpha values, and print their \\\\R^2\\\\ scores. You will use all\n",
                "of the features in the `sales_df` dataset to predict `\"sales\"`. The data\n",
                "has been split into `X_train`, `X_test`, `y_train`, `y_test` for you.\n",
                "\n",
                "A variable called `alphas` has been provided as a list containing\n",
                "different alpha values, which you will loop through to generate scores.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `Ridge`.\n",
                "- Instantiate `Ridge`, setting alpha equal to `alpha`.\n",
                "- Fit the model to the training data.\n",
                "- Calculate the \\\\R^2\\\\ score for each iteration of `ridge`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Ridge\n",
                "from sklearn.linear_model import Ridge\n",
                "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
                "ridge_scores = []\n",
                "for alpha in alphas:\n",
                "  \n",
                "  # Create a Ridge regression model\n",
                "  ridge = Ridge(alpha=alpha)\n",
                "  \n",
                "  # Fit the data\n",
                "  ridge.fit(X_train, y_train)\n",
                "  \n",
                "  # Obtain R-squared\n",
                "  score = ridge.score(X_test, y_test)\n",
                "  ridge_scores.append(score)\n",
                "print(ridge_scores)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lasso regression for feature importance\n",
                "\n",
                "In the video, you saw how lasso regression can be used to identify\n",
                "important features in a dataset.\n",
                "\n",
                "In this exercise, you will fit a lasso regression model to the\n",
                "`sales_df` data and plot the model's coefficients.\n",
                "\n",
                "The feature and target variable arrays have been pre-loaded as `X` and\n",
                "`y`, along with `sales_columns`, which contains the dataset's feature\n",
                "names.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `Lasso` from `sklearn.linear_model`.\n",
                "- Instantiate a Lasso regressor with an alpha of `0.3`.\n",
                "- Fit the model to the data.\n",
                "- Compute the model's coefficients, storing as `lasso_coef`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Lasso\n",
                "from sklearn.linear_model import Lasso\n",
                "\n",
                "# Instantiate a lasso regression model\n",
                "lasso = Lasso(alpha=0.3)\n",
                "\n",
                "# Fit the model to the data\n",
                "lasso.fit(X, y)\n",
                "\n",
                "# Compute and print the coefficients\n",
                "lasso_coef = lasso.coef_\n",
                "print(lasso_coef)\n",
                "plt.bar(sales_columns, lasso_coef)\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fine-Tuning Your Model\n",
                "\n",
                "### Assessing a diabetes prediction classifier\n",
                "\n",
                "In this chapter you'll work with the `diabetes_df` dataset introduced\n",
                "previously.\n",
                "\n",
                "The goal is to predict whether or not each individual is likely to have\n",
                "diabetes based on the features body mass index (BMI) and age (in years).\n",
                "Therefore, it is a binary classification problem. A target value of `0`\n",
                "indicates that the individual does *not* have diabetes, while a value of\n",
                "`1` indicates that the individual *does* have diabetes.\n",
                "\n",
                "`diabetes_df` has been preloaded for you as a pandas DataFrame and split\n",
                "into `X_train`, `X_test`, `y_train`, and `y_test`. In addition, a\n",
                "`KNeighborsClassifier()` has been instantiated and assigned to `knn`.\n",
                "\n",
                "You will fit the model, make predictions on the test set, then produce a\n",
                "confusion matrix and classification report.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `confusion_matrix` and `classification_report`.\n",
                "- Fit the model to the training data.\n",
                "- Predict the labels of the test set, storing the results as `y_pred`.\n",
                "- Compute and print the confusion matrix and classification report for\n",
                "  the test labels versus the predicted labels.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Â Import confusion matrix\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "knn = KNeighborsClassifier(n_neighbors=6)\n",
                "\n",
                "# Fit the model to the training data\n",
                "knn.fit(X_train, y_train)\n",
                "\n",
                "# Predict the labels of the test data: y_pred\n",
                "y_pred = knn.predict(X_test)\n",
                "\n",
                "# Generate the confusion matrix and classification report\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(classification_report(y_test, y_pred))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Building a logistic regression model\n",
                "\n",
                "In this exercise, you will build a logistic regression model using all\n",
                "features in the `diabetes_df` dataset. The model will be used to predict\n",
                "the probability of individuals in the test set having a diabetes\n",
                "diagnosis.\n",
                "\n",
                "The `diabetes_df` dataset has been split into `X_train`, `X_test`,\n",
                "`y_train`, and `y_test`, and preloaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `LogisticRegression`.\n",
                "- Instantiate a logistic regression model, `logreg`.\n",
                "- Fit the model to the training data.\n",
                "- Predict the probabilities of each individual in the test set having a\n",
                "  diabetes diagnosis, storing the array of positive probabilities as\n",
                "  `y_pred_probs`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Â Import LogisticRegression\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "# Instantiate the model\n",
                "logreg = LogisticRegression()\n",
                "\n",
                "# Fit the model\n",
                "logreg.fit(X_train, y_train)\n",
                "\n",
                "# Predict probabilities\n",
                "y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print(y_pred_probs[:10])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The ROC curve\n",
                "\n",
                "Now you have built a logistic regression model for predicting diabetes\n",
                "status, you can plot the ROC curve to visualize how the true positive\n",
                "rate and false positive rate vary as the decision threshold changes.\n",
                "\n",
                "The test labels, `y_test`, and the predicted probabilities of the test\n",
                "features belonging to the positive class, `y_pred_probs`, have been\n",
                "preloaded for you, along with `matplotlib.pyplot` as `plt`.\n",
                "\n",
                "You will create a ROC curve and then interpret the results.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `roc_curve`.\n",
                "- Calculate the ROC curve values, using `y_test` and `y_pred_probs`, and\n",
                "  unpacking the results into `fpr`, `tpr`, and `thresholds`.\n",
                "- Plot true positive rate against false positive rate.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import roc_curve\n",
                "from sklearn.metrics import roc_curve\n",
                "\n",
                "# Generate ROC curve values: fpr, tpr, thresholds\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "\n",
                "# Plot tpr against fpr\n",
                "plt.plot(fpr, tpr)\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve for Diabetes Prediction')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ROC AUC\n",
                "\n",
                "The ROC curve you plotted in the last exercise looked promising.\n",
                "\n",
                "Now you will compute the area under the ROC curve, along with the other\n",
                "classification metrics you have used previously.\n",
                "\n",
                "The `confusion_matrix` and `classification_report` functions have been\n",
                "preloaded for you, along with the `logreg` model you previously built,\n",
                "plus `X_train`, `X_test`, `y_train`, `y_test`. Also, the model's\n",
                "predicted test set labels are stored as `y_pred`, and probabilities of\n",
                "test set observations belonging to the positive class stored as\n",
                "`y_pred_probs`.\n",
                "\n",
                "A `knn` model has also been created and the performance metrics printed\n",
                "in the console, so you can compare the `roc_auc_score`,\n",
                "`confusion_matrix`, and `classification_report` between the two models.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `roc_auc_score`.\n",
                "- Calculate and print the ROC AUC score, passing the test labels and the\n",
                "  predicted positive class probabilities.\n",
                "- Calculate and print the confusion matrix.\n",
                "- Call `classification_report()`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import roc_auc_score\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "# Calculate roc_auc_score\n",
                "print(roc_auc_score(y_test, y_pred_probs))\n",
                "\n",
                "# Calculate the confusion matrix\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "\n",
                "# Calculate the classification report\n",
                "print(classification_report(y_test, y_pred))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparameter tuning with GridSearchCV\n",
                "\n",
                "Now you have seen how to perform grid search hyperparameter tuning, you\n",
                "are going to build a lasso regression model with optimal hyperparameters\n",
                "to predict blood glucose levels using the features in the `diabetes_df`\n",
                "dataset.\n",
                "\n",
                "`X_train`, `X_test`, `y_train`, and `y_test` have been preloaded for\n",
                "you. A `KFold()` object has been created and stored for you as `kf`,\n",
                "along with a lasso regression model as `lasso`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `GridSearchCV`.\n",
                "- Set up a parameter grid for `\"alpha\"`, using `np.linspace()` to create\n",
                "  20 evenly spaced values ranging from `0.00001` to `1`.\n",
                "- Call `GridSearchCV()`, passing `lasso`, the parameter grid, and\n",
                "  setting `cv` equal to `kf`.\n",
                "- Fit the grid search object to the training data to perform a\n",
                "  cross-validated grid search.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import GridSearchCV\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "#Â Set up the parameter grid\n",
                "param_grid = {\"alpha\": np.linspace(0.00001, 1, 20)}\n",
                "\n",
                "# Instantiate lasso_cv\n",
                "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
                "\n",
                "# Fit to the training data\n",
                "lasso_cv.fit(X_train, y_train)\n",
                "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
                "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparameter tuning with RandomizedSearchCV\n",
                "\n",
                "As you saw, `GridSearchCV` can be computationally expensive, especially\n",
                "if you are searching over a large hyperparameter space. In this case,\n",
                "you can use `RandomizedSearchCV`, which tests a fixed number of\n",
                "hyperparameter settings from specified probability distributions.\n",
                "\n",
                "Training and test sets from `diabetes_df` have been pre-loaded for you\n",
                "as `X_train`. `X_test`, `y_train`, and `y_test`, where the target is\n",
                "`\"diabetes\"`. A logistic regression model has been created and stored as\n",
                "`logreg`, as well as a `KFold` variable stored as `kf`.\n",
                "\n",
                "You will define a range of hyperparameters and use `RandomizedSearchCV`,\n",
                "which has been imported from `sklearn.model_selection`, to look for\n",
                "optimal hyperparameters from these options.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `params`, adding `\"l1\"` and `\"l2\"` as `penalty` values, setting\n",
                "  `C` to a range of `50` float values between `0.1` and `1.0`, and\n",
                "  `class_weight` to either `\"balanced\"` or a dictionary containing\n",
                "  `0:0.8, 1:0.2`.\n",
                "- Create the Randomized Search CV object, passing the model and the\n",
                "  parameters, and setting `cv` equal to `kf`.\n",
                "- Fit `logreg_cv` to the training data.\n",
                "- Print the model's best parameters and accuracy score.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Â Create the parameter space\n",
                "params = {\"penalty\": [\"l1\", \"l2\"],\n",
                "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
                "         \"C\": np.linspace(0.1, 1.0, 50),\n",
                "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
                "\n",
                "# Instantiate the RandomizedSearchCV object\n",
                "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
                "\n",
                "# Fit the data to the model\n",
                "logreg_cv.fit(X_train, y_train)\n",
                "\n",
                "# Print the tuned parameters and score\n",
                "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
                "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing and Pipelines\n",
                "\n",
                "### Creating dummy variables\n",
                "\n",
                "Being able to include categorical features in the model building process\n",
                "can enhance performance as they may add information that contributes to\n",
                "prediction accuracy.\n",
                "\n",
                "The `music_df` dataset has been preloaded for you, and its shape is\n",
                "printed. Also, `pandas` has been imported as `pd`.\n",
                "\n",
                "Now you will create a new DataFrame containing the original columns of\n",
                "`music_df` plus dummy variables from the `\"genre\"` column.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Use a relevant function, passing the entire `music_df` DataFrame, to\n",
                "  create `music_dummies`, dropping the first binary column.\n",
                "- Print the shape of `music_dummies`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create music_dummies\n",
                "music_dummies = pd.get_dummies(music_df, drop_first=True)\n",
                "\n",
                "# Print the new DataFrame's shape\n",
                "print(\"Shape of music_dummies: {}\".format(music_dummies.shape))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Regression with categorical features\n",
                "\n",
                "Now you have created `music_dummies`, containing binary features for\n",
                "each song's genre, it's time to build a ridge regression model to\n",
                "predict song popularity.\n",
                "\n",
                "`music_dummies` has been preloaded for you, along with `Ridge`,\n",
                "`cross_val_score`, `numpy` as `np`, and a `KFold` object stored as `kf`.\n",
                "\n",
                "The model will be evaluated by calculating the average RMSE, but first,\n",
                "you will need to convert the scores for each fold to positive values and\n",
                "take their square root. This metric shows the average error of our\n",
                "model's predictions, so it can be compared against the standard\n",
                "deviation of the target valueâ`\"popularity\"`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create `X`, containing all features in `music_dummies`, and `y`,\n",
                "  consisting of the `\"popularity\"` column, respectively.\n",
                "- Instantiate a ridge regression model, setting `alpha` equal to 0.2.\n",
                "- Perform cross-validation on `X` and `y` using the ridge model, setting\n",
                "  `cv` equal to `kf`, and using negative mean squared error as the\n",
                "  scoring metric.\n",
                "- Print the RMSE values by converting negative `scores` to positive and\n",
                "  taking the square root.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create X and y\n",
                "X = music_dummies.drop(\"popularity\", axis=1).values\n",
                "y = music_dummies[\"popularity\"].values\n",
                "\n",
                "#Â Instantiate a ridge model\n",
                "ridge = Ridge(alpha=0.2)\n",
                "\n",
                "#Â Perform cross-validation\n",
                "scores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
                "\n",
                "#Â Calculate RMSE\n",
                "rmse = np.sqrt(-scores)\n",
                "print(\"Average RMSE: {}\".format(np.mean(rmse)))\n",
                "print(\"Standard Deviation of the target array: {}\".format(np.std(y)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dropping missing data\n",
                "\n",
                "Over the next three exercises, you are going to tidy the `music_df`\n",
                "dataset. You will create a pipeline to impute missing values and build a\n",
                "KNN classifier model, then use it to predict whether a song is of the\n",
                "`\"Rock\"` genre.\n",
                "\n",
                "In this exercise specifically, you will drop missing values accounting\n",
                "for less than 5% of the dataset, and convert the `\"genre\"` column into a\n",
                "binary feature.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Print the number of missing values for each column in the `music_df`\n",
                "  dataset, sorted in ascending order.\n",
                "- Remove values for all columns with 50 or fewer missing values.\n",
                "- Convert `music_df[\"genre\"]` to values of `1` if the row contains `\"Rock\"`, otherwise change the value to `0`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print missing values for each column\n",
                "print(music_df.isna().sum().sort_values())\n",
                "\n",
                "# Remove values where less than 5% are missing\n",
                "music_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n",
                "\n",
                "# Convert genre to a binary feature\n",
                "music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n",
                "\n",
                "print(music_df.isna().sum().sort_values())\n",
                "print(\"Shape of the `music_df`: {}\".format(music_df.shape))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pipeline for song genre prediction: I\n",
                "\n",
                "Now it's time to build a pipeline. It will contain steps to impute\n",
                "missing values using the mean for each feature and build a KNN model for\n",
                "the classification of song genre.\n",
                "\n",
                "The modified `music_df` dataset that you created in the previous\n",
                "exercise has been preloaded for you, along with `KNeighborsClassifier`\n",
                "and `train_test_split`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `SimpleImputer` and `Pipeline`.\n",
                "- Instantiate an imputer.\n",
                "- Instantiate a KNN classifier with three neighbors.\n",
                "- Create `steps`, a list of tuples containing the imputer variable you\n",
                "  created, called `\"imputer\"`, followed by the `knn` model you created,\n",
                "  called `\"knn\"`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import modules\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Instantiate an imputer\n",
                "imputer = SimpleImputer()\n",
                "\n",
                "# Instantiate a knn model\n",
                "knn = KNeighborsClassifier(n_neighbors=3)\n",
                "\n",
                "# Build steps for the pipeline\n",
                "steps = [(\"imputer\", imputer), \n",
                "         (\"knn\", knn)]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pipeline for song genre prediction: II\n",
                "\n",
                "Having set up the steps of the pipeline in the previous exercise, you\n",
                "will now use it on the `music_df` dataset to classify the genre of\n",
                "songs. What makes pipelines so incredibly useful is the simple interface\n",
                "that they provide.\n",
                "\n",
                "`X_train`, `X_test`, `y_train`, and `y_test` have been preloaded for\n",
                "you, and `confusion_matrix` has been imported from `sklearn.metrics`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a pipeline using the steps you previously defined.\n",
                "- Fit the pipeline to the training data.\n",
                "- Make predictions on the test set.\n",
                "- Calculate and print the confusion matrix.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "steps = [(\"imputer\", imp_mean),\n",
                "        (\"knn\", knn)]\n",
                "\n",
                "# Create the pipeline\n",
                "pipeline = Pipeline(steps)\n",
                "\n",
                "# Fit the pipeline to the training data\n",
                "pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred = pipeline.predict(X_test)\n",
                "\n",
                "# Print the confusion matrix\n",
                "print(confusion_matrix(y_test, y_pred))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Centering and scaling for regression\n",
                "\n",
                "Now you have seen the benefits of scaling your data, you will use a\n",
                "pipeline to preprocess the `music_df` features and build a lasso\n",
                "regression model to predict a song's loudness.\n",
                "\n",
                "`X_train`, `X_test`, `y_train`, and `y_test` have been created from the\n",
                "`music_df` dataset, where the target is `\"loudness\"` and the features\n",
                "are all other columns in the dataset. `Lasso` and `Pipeline` have also\n",
                "been imported for you.\n",
                "\n",
                "Note that `\"genre\"` has been converted to a binary feature where `1`\n",
                "indicates a rock song, and `0` represents other genres.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `StandardScaler`.\n",
                "- Create the steps for the pipeline object, a `StandardScaler` object\n",
                "  called `\"scaler\"`, and a lasso model called `\"lasso\"` with `alpha` set\n",
                "  to `0.5`.\n",
                "- Instantiate a pipeline with steps to scale and build a lasso\n",
                "  regression model.\n",
                "- Calculate the R-squared value on the test data.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import StandardScaler\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Create pipeline steps\n",
                "steps = [(\"scaler\", StandardScaler()),\n",
                "         (\"lasso\", Lasso(alpha=0.5))]\n",
                "\n",
                "# Instantiate the pipeline\n",
                "pipeline = Pipeline(steps)\n",
                "pipeline.fit(X_train, y_train)\n",
                "\n",
                "#Â Calculate and print R-squared\n",
                "print(pipeline.score(X_test, y_test))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Centering and scaling for classification\n",
                "\n",
                "Now you will bring together scaling and model building into a pipeline\n",
                "for cross-validation.\n",
                "\n",
                "Your task is to build a pipeline to scale features in the `music_df`\n",
                "dataset and perform grid search cross-validation using a logistic\n",
                "regression model with different values for the hyperparameter `C`. The\n",
                "target variable here is `\"genre\"`, which contains binary values for rock\n",
                "as `1` and any other genre as `0`.\n",
                "\n",
                "`StandardScaler`, `LogisticRegression`, and `GridSearchCV` have all been\n",
                "imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Build the steps for the pipeline: a `StandardScaler()` object named\n",
                "  `\"scaler\"`, and a logistic regression model named `\"logreg\"`.\n",
                "- Create the `parameters`, searching 20 equally spaced float values\n",
                "  ranging from `0.001` to `1.0` for the logistic regression model's `C`\n",
                "  hyperparameter within the pipeline.\n",
                "- Instantiate the grid search object.\n",
                "- Fit the grid search object to the training data.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the steps\n",
                "steps = [(\"scaler\", StandardScaler()),\n",
                "         (\"logreg\", LogisticRegression())]\n",
                "pipeline = Pipeline(steps)\n",
                "\n",
                "# Create the parameter space\n",
                "parameters = {\"logreg__C\": np.linspace(0.001, 1.0, 20)}\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
                "                                                    random_state=21)\n",
                "\n",
                "# Instantiate the grid search object\n",
                "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
                "\n",
                "# Fit to the training data\n",
                "cv.fit(X_train, y_train)\n",
                "print(cv.best_score_, \"\\n\", cv.best_params_)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing regression model performance\n",
                "\n",
                "Now you have seen how to evaluate multiple models out of the box, you\n",
                "will build three regression models to predict a song's `\"energy\"`\n",
                "levels.\n",
                "\n",
                "The `music_df` dataset has had dummy variables for `\"genre\"` added.\n",
                "Also, feature and target arrays have been created, and these have been\n",
                "split into `X_train`, `X_test`, `y_train`, and `y_test`.\n",
                "\n",
                "The following have been imported for you: `LinearRegression`, `Ridge`,\n",
                "`Lasso`, `cross_val_score`, and `KFold`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Write a for loop using `model` as the iterator, and `model.values()`\n",
                "  as the iterable.\n",
                "- Perform cross-validation on the training features and the training\n",
                "  target array using the model, setting `cv` equal to the `KFold`\n",
                "  object.\n",
                "- Append the model's cross-validation scores to the results list.\n",
                "- Create a box plot displaying the results, with the x-axis labels as\n",
                "  the names of the models.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\n",
                "results = []\n",
                "\n",
                "# Loop through the models' values\n",
                "for model in models.values():\n",
                "  kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
                "  \n",
                "  # Perform cross-validation\n",
                "  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n",
                "  \n",
                "  # Append the results\n",
                "  results.append(cv_scores)\n",
                "  \n",
                "# Create a box plot of the results\n",
                "plt.boxplot(results, labels=models.keys())\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Predicting on the test set\n",
                "\n",
                "In the last exercise, linear regression and ridge appeared to produce\n",
                "similar results. It would be appropriate to select either of those\n",
                "models; however, you can check predictive performance on the test set to\n",
                "see if either one can outperform the other.\n",
                "\n",
                "You will use root mean squared error (RMSE) as the metric. The\n",
                "dictionary `models`, containing the names and instances of the two\n",
                "models, has been preloaded for you along with the training and target\n",
                "arrays `X_train_scaled`, `X_test_scaled`, `y_train`, and `y_test`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `mean_squared_error`.\n",
                "- Fit the model to the scaled training features and the training labels.\n",
                "- Make predictions using the scaled test features.\n",
                "- Calculate RMSE by passing the test set labels and the predicted\n",
                "  labels.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import mean_squared_error\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "for name, model in models.items():\n",
                "  \n",
                "  # Fit the model to the training data\n",
                "  model.fit(X_train_scaled, y_train)\n",
                "  \n",
                "  # Make predictions on the test set\n",
                "  y_pred = model.predict(X_test_scaled)\n",
                "  \n",
                "  # Calculate the test_rmse\n",
                "  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
                "  print(\"{} Test Set RMSE: {}\".format(name, test_rmse))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing classification model performance\n",
                "\n",
                "In this exercise, you will be solving a classification problem where the\n",
                "`\"popularity\"` column in the `music_df` dataset has been converted to\n",
                "binary values, with `1` representing popularity more than or equal to\n",
                "the median for the `\"popularity\"` column, and `0` indicating popularity\n",
                "below the median.\n",
                "\n",
                "Your task is to build and visualize the results of three different\n",
                "models to classify whether a song is popular or not.\n",
                "\n",
                "The data has been split, scaled, and preloaded for you as\n",
                "`X_train_scaled`, `X_test_scaled`, `y_train`, and `y_test`.\n",
                "Additionally, `KNeighborsClassifier`, `DecisionTreeClassifier`, and\n",
                "`LogisticRegression` have been imported.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a dictionary of `\"Logistic Regression\"`, `\"KNN\"`, and\n",
                "  `\"Decision Tree Classifier\"`, setting the dictionary's values to a\n",
                "  call of each model.\n",
                "- Loop through the values in `models`.\n",
                "- Instantiate a `KFold` object to perform 6 splits, setting `shuffle` to\n",
                "  `True` and `random_state` to `12`.\n",
                "- Perform cross-validation using the model, the scaled training\n",
                "  features, the target training set, and setting `cv` equal to `kf`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Â Create models dictionary\n",
                "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\n",
                "results = []\n",
                "\n",
                "# Loop through the models' values\n",
                "for model in models.values():\n",
                "  \n",
                "  #Â Instantiate a KFold object\n",
                "  kf = KFold(n_splits=6, random_state=12, shuffle=True)\n",
                "  \n",
                "  # Perform cross-validation\n",
                "  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
                "  results.append(cv_results)\n",
                "plt.boxplot(results, labels=models.keys())\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pipeline for predicting song popularity\n",
                "\n",
                "For the final exercise, you will build a pipeline to impute missing\n",
                "values, scale features, and perform hyperparameter tuning of a logistic\n",
                "regression model. The aim is to find the best parameters and accuracy\n",
                "when predicting song genre!\n",
                "\n",
                "All the models and objects required to build the pipeline have been\n",
                "preloaded for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create the steps for the pipeline by calling a simple imputer, a\n",
                "  standard scaler, and a logistic regression model.\n",
                "- Create a pipeline object, and pass the `steps` variable.\n",
                "- Instantiate a grid search object to perform cross-validation using the\n",
                "  pipeline and the parameters.\n",
                "- Print the best parameters and compute and print the test set accuracy\n",
                "  score for the grid search object.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create steps\n",
                "steps = [(\"imp_mean\", SimpleImputer()), \n",
                "         (\"scaler\", StandardScaler()), \n",
                "         (\"logreg\", LogisticRegression())]\n",
                "\n",
                "# Set up pipeline\n",
                "pipeline = Pipeline(steps)\n",
                "params = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n",
                "         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n",
                "\n",
                "# Create the GridSearchCV object\n",
                "tuning = GridSearchCV(pipeline, param_grid=params)\n",
                "tuning.fit(X_train, y_train)\n",
                "y_pred = tuning.predict(X_test)\n",
                "\n",
                "# Compute and print performance\n",
                "print(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
